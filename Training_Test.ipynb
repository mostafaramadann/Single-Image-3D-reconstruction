{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "key-point5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwL2wANkjxqA"
      },
      "source": [
        "!pip install tensorflow-addons\n",
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM7QRhqSx8g8"
      },
      "source": [
        "# **Importing Modules & Downloading Data**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhH2_8JduyX4"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import math\n",
        "import random\n",
        "import scipy.io\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import urllib.request\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from random import randint\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from packaging import version\n",
        "import tensorflow_addons as tfa\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(tf.__version__)\n",
        "print(np.__version__)\n",
        "print(tf.keras.__version__)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnC88NlywyFu"
      },
      "source": [
        "if not os.path.isfile(\"check_file\"):\n",
        "  !rm -rf sample_data\n",
        "  !wget --no-check-certificate -O keypoint-5.zip \"https://onedrive.live.com/download?cid=82C508E2119FA196&resid=82C508E2119FA196%218384&authkey=AILIjeUiynP-fv0\"\n",
        "  !unzip keypoint-5.zip\n",
        "  !rm -f keypoint-5.zip\n",
        "  !touch check_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj00zQoCyFx2"
      },
      "source": [
        "# **Configurations**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aocgrWHyFYE"
      },
      "source": [
        "HEATMAP_SIZE = [40,30]\n",
        "kp = None\n",
        "annotator = 1 ## 0 for avg, 1-3 for other annotators\n",
        "path_index = 0 ## 0 BED, 1 CHAIR, 2 SOFA, 3 SWIVELCHAIR,4 TABLE\n",
        "hmaps = \"KDE\" ## the heatmap label type GAUSSIAN ## NORMAL ## SCORE ## KDE\n",
        "epochs = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjlDD4ZE4Ohs"
      },
      "source": [
        "w_ct = 4\n",
        "if path_index == 2 or path_index == 3:\n",
        "  w_ct = 6\n",
        "  \n",
        "if path_index == 2:\n",
        "  kp = 14\n",
        "elif path_index == 3:\n",
        "  kp = 13\n",
        "elif path_index == 4:\n",
        "  kp = 8\n",
        "else:\n",
        "  kp = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDEId1YpSNhk"
      },
      "source": [
        "# **Data Cleaning & Preprocessing**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ95FmVtw0UI"
      },
      "source": [
        "dir_path = os.getcwd()\n",
        "categories = [\"bed\",\"chair\",\"sofa\",\"swivelchair\",\"table\"]\n",
        "paths=[]\n",
        "for category in categories:\n",
        "    paths.append(os.path.join(dir_path,category))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-PnDGiiw9rt"
      },
      "source": [
        "def avg_keypoint(points:list)->int:\n",
        "    \n",
        "    sums=[]\n",
        "    for point in points:\n",
        "        if point!=0 and not np.isnan(np.sum(point)):\n",
        "            sums.append(point)\n",
        "    try:        \n",
        "        return sum(sums)/len(sums)\n",
        "    except ArithmeticError:\n",
        "        return -1\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFfGHllhxPkU"
      },
      "source": [
        "def get_keypoints(matrix,avg=True,annotator=0):\n",
        "    x_major = []\n",
        "    y_major = []\n",
        "    for i in range(matrix.shape[3]):\n",
        "        x_minor=[]\n",
        "        y_minor=[]\n",
        "        for p in range(matrix.shape[1]):\n",
        "            \n",
        "            if avg:\n",
        "                x = avg_keypoint(matrix[0,p,:,i])\n",
        "                y = avg_keypoint(matrix[1,p,:,i])\n",
        "            else:\n",
        "                x = matrix[0,p,annotator,i]\n",
        "                y = matrix[1,p,annotator,i]    \n",
        "                if np.isnan(np.sum(x)) or np.isnan(np.sum(y)):\n",
        "                    x, y = -1,-1\n",
        "                    \n",
        "            x_minor.append(int(x))\n",
        "            y_minor.append(int(y))\n",
        "                \n",
        "        if len(x_minor)!=0 and len(y_minor)!=0:\n",
        "            x_major.append(x_minor)\n",
        "            y_major.append(y_minor)\n",
        "            \n",
        "    return np.array([x_major,y_major]).transpose(1,2,0)#examples,keypoints,(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x63lv5sQNGWV"
      },
      "source": [
        "def local_contrast_normalize(image,kernel_shape=(1,3,9,9),sigma=2):\n",
        "    \"\"\"\n",
        "    \n",
        "    Kernel_shape: tuple contains following (out_chan,in_chan,height,width)\n",
        "    image: nd numpy array with shape (height,width,channels)\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    height,width,channels = image.shape\n",
        "    x = np.zeros(kernel_shape,dtype=\"float32\")\n",
        "    Z = 1/(2 * np.pi * sigma ** 2)\n",
        "    mid = int(kernel_shape[-1]/2)\n",
        "    \n",
        "    for channel in range(kernel_shape[1]):\n",
        "        for row in range(kernel_shape[2]):\n",
        "            for column in range(kernel_shape[3]):\n",
        "                coords = -((column-mid)**2 + (row-mid)**2)            \n",
        "                x[0,channel,row,column] = Z*np.exp(coords/(2*sigma**2))\n",
        "                \n",
        "    kernel = (x/np.sum(x)).transpose(2,3,1,0) # kernel shape is (h,w,chan_in,chan_out)\n",
        "    image = image.reshape((1,height,width,channels)).astype('float32')#image shape is (batch_size,h,w,channels)\n",
        "    \n",
        "    paddings = tf.constant([[0,0],[8,8],[8,8],[0,0]])\n",
        "    padded = tf.pad(tf.constant(image),paddings,\"CONSTANT\")\n",
        "\n",
        "    mid = int(np.floor(kernel.shape[1] / 2.))\n",
        "    blurred = tf.nn.conv2d(padded,tf.constant(kernel),[1,1,1,1],\"VALID\")\n",
        "    \n",
        "    centered = (image-blurred[:,mid:-mid,mid:-mid,:]).numpy()\n",
        "\n",
        "    normalized = ((centered-centered.min())/(centered.max()-centered.min()))\n",
        "    \n",
        "    return np.squeeze(normalized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTSUXwALxSK-"
      },
      "source": [
        "def get_data(path,train=True,size=(320,240),lcn=False):\n",
        "    \n",
        "    if train:\n",
        "        data_path = os.path.join(path,\"train.txt\")\n",
        "    else:\n",
        "        data_path = os.path.join(path,\"test.txt\")\n",
        "\n",
        "    data=[]\n",
        "    with open(data_path) as file:\n",
        "        images_path = file.readlines()\n",
        "        \n",
        "        for image_path in images_path:\n",
        "            image_path = image_path.split(\"/\")[-1]\n",
        "            \n",
        "            full_path = os.path.join(path,\"images\",image_path)\n",
        "            \n",
        "            if full_path[-1] == \"\\n\":\n",
        "                full_path = full_path[:-1]\n",
        "\n",
        "            image = plt.imread(full_path)\n",
        "            if len(image.shape)<3:\n",
        "              image = cv.cvtColor(image,cv.COLOR_GRAY2RGB)\n",
        "\n",
        "            if lcn:\n",
        "                image = local_contrast_normalize(image)\n",
        "            \n",
        "            while image.shape[0]<size[1]-20:\n",
        "              image = cv.pyrUp(image)\n",
        "            while image.shape[0]>size[1]+20:\n",
        "              image = cv.pyrDown(image)\n",
        "            image = cv.resize(image,size)\n",
        "            \n",
        "            \n",
        "            norm  = (image-image.min())/(image.max()-image.min())\n",
        "            data.append(norm)\n",
        "            \n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ2nHoBCxtb4"
      },
      "source": [
        "def get_labels(path,category,train=True):\n",
        "   \n",
        "    if train:\n",
        "        data_path = os.path.join(path,\"train.txt\")\n",
        "    else:\n",
        "        data_path = os.path.join(path,\"test.txt\")\n",
        "        \n",
        "    labels = []\n",
        "    \n",
        "    with open(data_path) as file:\n",
        "        images_path = file.readlines()\n",
        "\n",
        "        for image_path in images_path:\n",
        "            \n",
        "            full_path = image_path.split(\"/\")[-1]## number of image.jpg\n",
        "            number = full_path.split(\".\")[0]### without.jpg\n",
        "            \n",
        "            image_path = int(number)##casted\n",
        "            full_path = os.path.join(path,\"images\",full_path)##full image path\n",
        "            \n",
        "            \n",
        "            if full_path[-1] == \"\\n\":\n",
        "                full_path = full_path[:-1]\n",
        "                \n",
        "        \n",
        "            image = plt.imread(full_path)\n",
        "            \n",
        "            factor_h = image.shape[0]/40\n",
        "            factor_w = image.shape[1]/30\n",
        "            \n",
        "            i=0\n",
        "            for x_point,y_point in zip(category[image_path-1,:,0],category[image_path-1,:,1]):\n",
        "                \n",
        "                if category[image_path-1,i,0]!=-1 and category[image_path-1,i,1]!=-1:\n",
        "                    \n",
        "                    category[image_path-1,i,1]=int(category[image_path-1,i,1]/factor_h)\n",
        "                    category[image_path-1,i,0]=int(category[image_path-1,i,0]/factor_w)\n",
        "                    \n",
        "                i+=1\n",
        "                \n",
        "            labels.append(category[image_path-1])\n",
        "            \n",
        "    return np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHBYMsnI1xKr"
      },
      "source": [
        "mat = scipy.io.loadmat(os.path.join(paths[path_index],\"coords.mat\"))[\"coords\"]\n",
        "\n",
        "mats =list()\n",
        "mats.append(get_keypoints(mat,avg=True))\n",
        "for i in range(3):\n",
        "    mats.append(get_keypoints(mat,avg=False,annotator=i))\n",
        "                  \n",
        "Y_points_train = list()\n",
        "Y_points_test = list()\n",
        "for i in range(4):\n",
        "    Y_points_train.append(get_labels(paths[path_index],mats[i]))\n",
        "    Y_points_test.append(get_labels(paths[path_index],mats[i],train=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI0Tuq8w2SWC"
      },
      "source": [
        "train_labels = np.array(Y_points_train)\n",
        "test_labels = np.array(Y_points_test) \n",
        "print(train_labels.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7e4L3Nq5Wmx"
      },
      "source": [
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaEMhySzxuon"
      },
      "source": [
        "X_train1 = get_data(paths[path_index],size=(240,320))\n",
        "X_train2 = get_data(paths[path_index],size=(120,160))\n",
        "X_train3 = get_data(paths[path_index],size=(60,80))\n",
        "X_train = [X_train1,X_train2,X_train3]\n",
        "\n",
        "X_test1 = np.array(get_data(paths[path_index],size=(240,320),train=False))\n",
        "X_test2 = np.array(get_data(paths[path_index],size=(120,160),train=False))\n",
        "X_test3 = np.array(get_data(paths[path_index],size=(60,80),train=False))\n",
        "X_test=[X_test1,X_test2,X_test3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7N1fex7TihC"
      },
      "source": [
        "# **HeatMaps**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhYeJuVfcd2p"
      },
      "source": [
        "# def clustering_heatmap(data):\n",
        "#   def get_mid(x):\n",
        "#     i,mid,coordinates = x\n",
        "#     return mid\n",
        "#   keypoints = data.shape[-2]\n",
        "#   left = [] ## array with indices of midpoint less than mid width of image\n",
        "#   right = []## array with indices of midpoint greater than mid width of image\n",
        "\n",
        " \n",
        "#   for i in range(data.shape[0]):\n",
        "#     coordinates = data[i].copy()\n",
        "#     ### translate_coords with respect to 8,9\n",
        "#     xt1,_= coordinates[8]\n",
        "#     xt2,_ = coordinates[9]\n",
        "#     mid_chair = int((xt1+xt2)/2)\n",
        "#     translation_value = int(HEATMAP_SIZE[1]/2)-mid_chair\n",
        "\n",
        "#     coordinates[:,0] +=translation_value \n",
        "#     x1,_ = coordinates[0]\n",
        "#     x2,_ = coordinates[1]\n",
        "#     coordinates[:,0] -=translation_value\n",
        "#     mid = int(round((x1+x2)/2))\n",
        "#     if mid>int(HEATMAP_SIZE[1]/2):\n",
        "#       right.append((i,mid,coordinates))\n",
        "#     else:\n",
        "#       left.append((i,mid,coordinates))\n",
        "  \n",
        "#   left.sort(key=get_mid)\n",
        "#   right.sort(key=get_mid)\n",
        "\n",
        "#   ## left and right is distributed into four quarters\n",
        "#   left_distribution_heatmap = np.zeros((HEATMAP_SIZE[0],HEATMAP_SIZE[1],keypoints),dtype=np.float32)\n",
        "#   right_distribution_heatmap = np.zeros((HEATMAP_SIZE[0],HEATMAP_SIZE[1],keypoints),dtype=np.float32)\n",
        "#   for item in left:\n",
        "#     _,_,coordinates = item\n",
        "#     for i,coord in enumerate(coordinates):\n",
        "#       x,y = coord\n",
        "#       while x >= HEATMAP_SIZE[1]:\n",
        "#         x-=1\n",
        "#       while y >= HEATMAP_SIZE[0]:\n",
        "#         y-=1\n",
        "#       left_distribution_heatmap[y,x,i]+=5\n",
        "\n",
        "#   for item in right:\n",
        "#     _,_,coordinates = item\n",
        "#     for i,coord in enumerate(coordinates):\n",
        "#       x,y = coord\n",
        "#       while x >= HEATMAP_SIZE[1]:\n",
        "#         x-=1\n",
        "#       while y >= HEATMAP_SIZE[0]:\n",
        "#         y-=1\n",
        "#       left_distribution_heatmap[y,x,i]+=5\n",
        "\n",
        "#   left_distribution_heatmap = ((left_distribution_heatmap-left_distribution_heatmap.min())\n",
        "#                                 /(left_distribution_heatmap.max()-left_distribution_heatmap.min()))*0.25\n",
        "    \n",
        "  \n",
        "#   right_distribution_heatmap = ((right_distribution_heatmap-right_distribution_heatmap.min())\n",
        "#                                 /(right_distribution_heatmap.max()-right_distribution_heatmap.min()))*0.25\n",
        "\n",
        "\n",
        "#   return [left,right]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3VjnDrSzTTI"
      },
      "source": [
        "# left,right = clustering_heatmap(train_labels[annotator])\n",
        "# print(len(left))\n",
        "# print(len(right))\n",
        "# for i in range(750,799):\n",
        "#   index,mid,coordinates = left[i]\n",
        "#   plt.figure(figsize=(8,8))\n",
        "#   plt.imshow(X_train[0][index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vTwecwj7oDP"
      },
      "source": [
        "# ### back&front chairs could be solved by slopes\n",
        "# for i in range(900,949):\n",
        "#   index,mid,coordinates = right[i]\n",
        "#   plt.figure(figsize=(8,8))\n",
        "#   heatmap = np.zeros((HEATMAP_SIZE[0],HEATMAP_SIZE[1]))\n",
        "#   heatmap[coordinates[0,1]-1,mid+1] = 255\n",
        "#   heatmap = tf.image.resize(np.expand_dims(heatmap,axis=-1),[X_train[0][index].shape[0],X_train[0][index].shape[1]])\n",
        "#   plt.imshow(X_train[0][index]+heatmap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN9x4R4949WA"
      },
      "source": [
        "def scoring_sum(labels,keypoints_num):\n",
        "    heatmaps = list()\n",
        "    for example in range(labels[0].shape[0]):\n",
        "        main_heatmap = np.zeros(shape=(40,30,keypoints_num),dtype=\"float32\")\n",
        "        for label in labels:\n",
        "            i=0\n",
        "            for x,y in zip(label[example,:,0],label[example,:,1]):\n",
        "                if x!=-1 and y!=-1 and x<=29 and y<=39:\n",
        "                  main_heatmap[y,x,i]+=5\n",
        "                i+=1      \n",
        "        norm = (main_heatmap-main_heatmap.min())/(main_heatmap.max()-main_heatmap.min())                \n",
        "        heatmaps.append(norm)\n",
        "        \n",
        "    return np.array(heatmaps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LZWtqrOzJeY"
      },
      "source": [
        "def normal_heatmap(data,kp_ct):\n",
        "  keypoints = data.shape[-2]\n",
        "  heatmaps = []\n",
        "  for i in range(data.shape[0]):\n",
        "    heatmap = np.zeros(shape=(HEATMAP_SIZE[0],HEATMAP_SIZE[1],kp_ct),dtype=np.float32)\n",
        "    coordinates = data[i]\n",
        "    k = 0\n",
        "    for x,y in coordinates:\n",
        "      if k == kp_ct:\n",
        "        break\n",
        "      if x>=0 and y>=0 and x<=HEATMAP_SIZE[1] and y<=HEATMAP_SIZE[0]:\n",
        "        if x == HEATMAP_SIZE[1]:\n",
        "          x-=1\n",
        "        if y == HEATMAP_SIZE[0]:\n",
        "          y-=1\n",
        "        heatmap[y,x,k] = 1\n",
        "      k+=1\n",
        "    heatmaps.append(heatmap)\n",
        "  \n",
        "  heatmaps = np.array(heatmaps)\n",
        "  normalized = (heatmaps-heatmaps.min())/(heatmaps.max()-heatmaps.min())\n",
        "  return heatmaps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHHwz-tPzKyu"
      },
      "source": [
        "def gaussian_heatmap(data,sigma=1.0,key_points=-1,on_point=False):\n",
        "\n",
        "  Z = 1/(2 * np.pi * sigma ** 2)\n",
        "  if key_points == -1:\n",
        "    key_points = data.shape[-2]\n",
        "\n",
        "  heatmaps = []  \n",
        "  \n",
        "  for example in data:\n",
        "    midy = int(HEATMAP_SIZE[0]/2) \n",
        "    midx = int(HEATMAP_SIZE[1]/2)\n",
        "\n",
        "    k=0\n",
        "    heatmap = np.zeros((HEATMAP_SIZE[0],HEATMAP_SIZE[1],key_points),dtype=np.float32)\n",
        "    for key_point in example:\n",
        "      if k == key_points and key_points!=-1:\n",
        "        break\n",
        "      x,y = key_point\n",
        "      if x == HEATMAP_SIZE[1]:\n",
        "        x-=1\n",
        "      if y == HEATMAP_SIZE[0]:\n",
        "        y-=1\n",
        "      if on_point:\n",
        "        ## AS WE GO FURTHER DECREASE SCORE ## \n",
        "        midx,midy = x,y\n",
        "        if midx>0 and midy>0:\n",
        "          for row in range(1,HEATMAP_SIZE[0]):\n",
        "            for column in range(1,HEATMAP_SIZE[1]):\n",
        "              c = -((column-midx)**2 + (row-midy)**2)\n",
        "              heatmap[row,column,k] = Z*np.exp(c/(2*sigma**2))\n",
        "        \n",
        "      else:\n",
        "        ## USE CENTER OF IMAGE APPROACH ## \n",
        "        if x>=0 and y>=0 and x<=HEATMAP_SIZE[1] and y<=HEATMAP_SIZE[0]:\n",
        "          c = -((x-midx)**2 + (y-midy)**2)\n",
        "          heatmap[y,x,k] = Z*np.exp(c/(2*sigma**2))\n",
        "      heatmap[0,0,k] = 0\n",
        "      k+=1\n",
        "    heatmap/=np.sum(heatmap)\n",
        "    heatmaps.append(heatmap)\n",
        "\n",
        "  heatmaps = np.array(heatmaps)\n",
        "  normalized = (heatmaps-heatmaps.min())/(heatmaps.max()-heatmaps.min())\n",
        "  return normalized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njcy8FGpz222"
      },
      "source": [
        "def visualize_heatmap(images,map,index,key_point,all=False,resize=False):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    image: an array of (B,W,H,C)\n",
        "    map: an array of (B,W,H,K) where key is the number of key-points\n",
        "    index: an index for the image\n",
        "    key_point: an index for the key_point\n",
        "  \"\"\"\n",
        "\n",
        "  heatmap = None\n",
        "  image = images[index]\n",
        "\n",
        "  if all:\n",
        "    ## Generating complete heatmap\n",
        "    ### when generating all use non-max_suppression\n",
        "    ### make sure each keypoint has constant intensity value\n",
        "    complete_map = np.zeros((map.shape[1],map.shape[2]))\n",
        "\n",
        "    for i in range(map.shape[-1]):\n",
        "      minimap = map[index,:,:,i].copy()\n",
        "\n",
        "      max_value = minimap.max()\n",
        "      \n",
        "      minimap[minimap<max_value]=0\n",
        "      r = tf.argmax(tf.argmax(minimap,axis=1))\n",
        "      c = tf.argmax(tf.argmax(minimap,axis=0))\n",
        "      minimap[r,c] = 1.0\n",
        "      complete_map+=minimap\n",
        "\n",
        "    heatmap = complete_map\n",
        "  else:\n",
        "    heatmap = map[index,:,:,key_point]\n",
        "  \n",
        "  if resize:\n",
        "    heatmap = np.expand_dims(heatmap,-1)\n",
        "    heatmap = tf.image.resize(heatmap,[image.shape[0],image.shape[1]])\n",
        "    heatmap = np.squeeze(heatmap,axis=-1)\n",
        "\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "  ax = axes.ravel()\n",
        "\n",
        "  ax[0].imshow(image)\n",
        "  ax[0].set_title(\"Original\")\n",
        "\n",
        "  ax[1].imshow(image, vmin=0, vmax=2)\n",
        "  ax[1].imshow(heatmap, alpha=0.8,cmap=\"gray\")\n",
        "  ax[1].set_title(\"Original + heatmap\")\n",
        "\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "  plt.figure(figsize=(20,20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8F6BeiZilGf"
      },
      "source": [
        "def rotate_randomly(X_train,heatmaps):\n",
        "  shape = len(X_train[0])\n",
        "  scales = dict()\n",
        "\n",
        "  for i in range(3):\n",
        "    scales[i] = []\n",
        "\n",
        "  r_heatmaps = []\n",
        "  for i in range(shape-1,-1,-1):\n",
        "    rotation_angle = random.randint(-1,1)#tf.random.uniform([1],minval=-5,maxval=5,dtype=tf.int32)\n",
        "    probability = tf.random.uniform([1],minval=0,maxval=1)[0] \n",
        "\n",
        "    if probability > 0.9 and (rotation_angle<0 or rotation_angle>0):\n",
        "\n",
        "      rotation_angle = tf.cast((rotation_angle/10),tf.float32)\n",
        "      for k in range(3):\n",
        "        scale = tfa.image.rotate(X_train[k][i],rotation_angle,interpolation=\"bilinear\")\n",
        "        scales[k].append(scale)\n",
        "\n",
        "      rotated_heatmap = tfa.image.rotate(heatmaps[i],rotation_angle,interpolation=\"bilinear\")\n",
        "      r_heatmaps.append(rotated_heatmap)\n",
        "\n",
        "    elif probability > 0.85:\n",
        "      for k in range(3):\n",
        "        scale = tf.image.flip_left_right(X_train[k][i])\n",
        "        scales[k].append(scale)\n",
        "\n",
        "      flipped_heatmap = tf.image.flip_left_right(heatmaps[i])\n",
        "      r_heatmaps.append(flipped_heatmap)\n",
        "\n",
        "    if i == int(shape/4):\n",
        "      print(\"Done 75%\")\n",
        "    elif i == int(shape/2):\n",
        "      print(\"Done 50%\")\n",
        "    elif i == int(shape/4)*3:\n",
        "      print(\"Done 25%\")\n",
        "  \n",
        "  # for i in range(3):\n",
        "  #   scales[i] = np.array(scales[i])\n",
        "\n",
        "  # return scales,np.array(r_heatmaps)\n",
        "  return scales,r_heatmaps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRABLgORzVxC"
      },
      "source": [
        "heatmaps = None\n",
        "test_heatmaps = None\n",
        "if hmaps == \"GAUSSIAN\":\n",
        "   heatmaps = gaussian_heatmap(train_labels[annotator],on_point=True,sigma=1.5,key_points=kp)\n",
        "   test_heatmaps = gaussian_heatmap(test_labels[annotator],on_point=True,sigma=1.5,key_points=kp)\n",
        "elif hmaps == \"KDE\":\n",
        "  for i in range(2):\n",
        "    if heatmaps is None:\n",
        "      heatmaps = gaussian_heatmap(train_labels[i],on_point=True,sigma=1.5,key_points=kp)\n",
        "      test_heatmaps = gaussian_heatmap(test_labels[i],on_point=True,sigma=1.5,key_points=kp)\n",
        "    else:\n",
        "      heatmaps = np.add(heatmaps,gaussian_heatmap(train_labels[i],on_point=True,sigma=1.5,key_points=kp))\n",
        "      test_heatmaps = np.add(test_heatmaps,gaussian_heatmap(test_labels[i],on_point=True,sigma=1.5,key_points=kp))\n",
        "      \n",
        "elif hmaps == \"NORMAL\":\n",
        "  heatmaps = normal_heatmap(train_labels[annotator],kp)\n",
        "  test_heatmaps = normal_heatmap(test_labels[annotator],kp)\n",
        "elif hmaps == \"SCORE\":\n",
        "  heatmaps = scoring_sum(train_labels,kp)\n",
        "  test_heatmaps = scoring_sum(test_labels,kp)\n",
        " \n",
        "heatmaps = (heatmaps-heatmaps.min())/(heatmaps.max()-heatmaps.min())\n",
        "test_heatmaps = (test_heatmaps-test_heatmaps.min())/(test_heatmaps.max()-test_heatmaps.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHO44IiVmNo4"
      },
      "source": [
        "scales,r_heatmaps = rotate_randomly(X_train,heatmaps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkes1WWaEzC8"
      },
      "source": [
        "for i in range(len(scales[0])):\n",
        "  index = random.randint(0,len(scales[0])-1)\n",
        "  for k in range(3):\n",
        "    X_train[k].insert(index,scales[k][i])\n",
        "  \n",
        "  heatmaps = np.insert(heatmaps,index,r_heatmaps[i],axis=0)\n",
        "\n",
        "del scales,r_heatmaps\n",
        "gc.collect()\n",
        "del  X_train1,X_train2,X_train3,train_labels,test_labels,mats\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iGwTF2V0FQM"
      },
      "source": [
        "for i in range(3):\n",
        "  X_test[i] = np.array(X_test[i])\n",
        "  X_train[i] = np.array(X_train[i])\n",
        "  print(X_train[i].max())\n",
        "print(heatmaps.max())\n",
        "print(X_test[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDyCEGg76qGX"
      },
      "source": [
        "print(heatmaps.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaRfdBxRTNd3"
      },
      "source": [
        "for i in range(40):\n",
        "  visualize_heatmap(X_train[0],heatmaps,i,0,all=True,resize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UbblRr4zxSS"
      },
      "source": [
        "for i in range(3):\n",
        "  print(X_train[i].shape)\n",
        "print(heatmaps.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrpHU4xp88-j"
      },
      "source": [
        "# **Model Building & Training**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxcPAE2G8UXQ"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Input,UpSampling2D,concatenate,Flatten,Dense,Conv2DTranspose,Cropping2D\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3kQ78sC9Cq9"
      },
      "source": [
        "class ConvMax(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self,filters,kernel_size,scale=None,last = False):\n",
        "    super(ConvMax, self).__init__()\n",
        "    self.conv = Conv2D(filters,kernel_size=kernel_size,activation='relu',padding=\"same\")\n",
        "    self.bn = BatchNormalization(center=True,scale=True,trainable=True)\n",
        "    self.pool = MaxPooling2D(pool_size=(2, 2))\n",
        "    self.last = last\n",
        "    self.scale = scale\n",
        "\n",
        "    if self.last:\n",
        "     self.conv2 = Conv2D(512,kernel_size = (9,9),activation=\"relu\",padding=\"same\") ### replace hard coded later\n",
        "     self.bn2 = BatchNormalization(center=True,scale=True,trainable=True)\n",
        "\n",
        "  \n",
        "  def call(self,input_tensor):\n",
        "    x = self.conv(input_tensor)\n",
        "    x = self.bn(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    if self.last:\n",
        "      x = self.conv2(x)\n",
        "      x = self.bn2(x)\n",
        "\n",
        "    if self.scale is not None:\n",
        "      x = tf.image.resize(x, [40,30]) ## replace hard coded scale later\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R89wXMJK9EhB"
      },
      "source": [
        "class ScaleBlock(tf.keras.Model):\n",
        "    def __init__(self, filters, kernel_size,scale):\n",
        "        super(ScaleBlock, self).__init__()\n",
        "\n",
        "        self.convmax1 = ConvMax(filters[0],kernel_size)\n",
        "        self.convmax2 = ConvMax(filters[1],kernel_size)\n",
        "        self.convmax3 = ConvMax(filters[2],kernel_size,last=True,scale=scale)\n",
        "    \n",
        "    def call(self, input_tensor):\n",
        "      x = self.convmax1(input_tensor)\n",
        "      x = self.convmax2(x)\n",
        "      x = self.convmax3(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCGQ8gPa9G1-"
      },
      "source": [
        "class Model(tf.keras.Model):\n",
        "\n",
        "  def __init__(self,filters,kernel_size,keypoints,regression_filter=512):\n",
        "\n",
        "    super(Model, self).__init__()\n",
        "    self.scaleb1 = ScaleBlock(filters,kernel_size,scale=False)\n",
        "    self.scaleb2 = ScaleBlock(filters,kernel_size,scale=True)\n",
        "    self.scaleb3 = ScaleBlock(filters,kernel_size,scale=True)\n",
        "\n",
        "    self.bn1 = BatchNormalization(center=True,scale=True,trainable=True)\n",
        "\n",
        "    self.conv1 = Conv2D(regression_filter,kernel_size=(1,1),padding=\"same\",activation = \"relu\")\n",
        "    self.bn2 = BatchNormalization(center=True,scale=True,trainable=True)\n",
        "\n",
        "    self.conv2 = Conv2D(regression_filter,kernel_size=(1,1),padding=\"same\",activation=\"relu\")\n",
        "    self.bn3 = BatchNormalization(center=True,scale=True,trainable=True)\n",
        "\n",
        "    self.conv_key = Conv2D(keypoints,kernel_size=(1,1),padding=\"same\",activation=\"sigmoid\")\n",
        "\n",
        "    self.encoder = tf.keras.Sequential(\n",
        "        [\n",
        "         Flatten(),\n",
        "         Dense(8192,activation=\"relu\"),\n",
        "         Dense(4096,activation=\"relu\")\n",
        "        ]\n",
        "     )\n",
        "    self.decoder = tf.keras.Sequential(\n",
        "         [\n",
        "          Dense(8192,activation=\"relu\"),\n",
        "          Dense(40*30*keypoints,activation=\"sigmoid\"),\n",
        "          tf.keras.layers.Reshape((40,30,keypoints)),\n",
        "         ]\n",
        "     ) \n",
        "    self.check=True\n",
        "    \n",
        "\n",
        "  def call(self,input_tensor):\n",
        "    x1 = self.scaleb1(input_tensor[0])\n",
        "    x2 = self.scaleb2(input_tensor[1])\n",
        "    x3 = self.scaleb3(input_tensor[2])\n",
        "\n",
        "    X = tf.keras.layers.add([x1,x2,x3])\n",
        "    X = self.bn1(X)\n",
        "\n",
        "    X = self.conv1(X)\n",
        "    X = self.bn2(X)\n",
        "\n",
        "    X = self.conv2(X)\n",
        "    X = self.bn3(X)\n",
        "\n",
        "    noisy_heatmap = self.conv_key(X)\n",
        "    encoded_heatmap = self.encoder(noisy_heatmap)\n",
        "    soft_heatmap = self.decoder(encoded_heatmap)\n",
        "\n",
        "    if soft_heatmap.shape[1] == 40 and soft_heatmap.shape[2] == 28:\n",
        "      soft_heatmap = tf.image.resize(soft_heatmap,[40,30])\n",
        "\n",
        "    if self.check:\n",
        "      #print(soft_heatmap.shape[1],soft_heatmap.shape[2])\n",
        "      self.check = False\n",
        "      assert soft_heatmap.shape[1] == 40 and soft_heatmap.shape[2] == 30\n",
        "\n",
        "  \n",
        "    return soft_heatmap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7A4eom26cv7"
      },
      "source": [
        "def avg_error(y_true,y_pred):\n",
        "  key_points_error = []\n",
        "\n",
        "  for i in range(y_pred.shape[-1]):\n",
        "    key_points_error.append(0)\n",
        "\n",
        "  key_points_error = np.array(key_points_error)\n",
        "\n",
        "  for i in range(y_pred.shape[0]):\n",
        "    prediction = y_pred[i]\n",
        "    ground_truth = y_true[i]\n",
        "\n",
        "    for k in range(prediction.shape[-1]):\n",
        "      pred_map = prediction[:,:,k].numpy()\n",
        "      truth_map = ground_truth[:,:,k].numpy()\n",
        "\n",
        "      r_pred,c_pred = np.unravel_index(pred_map.argmax(),pred_map.shape)\n",
        "      r_true,c_true = np.unravel_index(truth_map.argmax(),truth_map.shape)\n",
        "      \n",
        "      dy = tf.abs(r_true-r_pred)\n",
        "      dx = tf.abs(c_true-c_pred)\n",
        "      error = dx+dy\n",
        "      key_points_error[k]+=error\n",
        "\n",
        "  avg_error = key_points_error/y_pred.shape[-1]\n",
        "  return (tf.reduce_sum(avg_error)/y_pred.shape[0]) ## average error of all key_points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhb3FdU5CM9A"
      },
      "source": [
        "# #@tf.function ### annotaion makes code run in graph mode\n",
        "# def train_step(model,optimizer,loss_object,images,hmaps):\n",
        "#   with tf.GradientTape() as tape:\n",
        "#     predicted_heatmaps = model(images,training = True)\n",
        "#     loss = loss_object(hmaps,predicted_heatmaps)## batch_loss\n",
        "#   gradients = tape.gradient(loss,model.trainable_variables) ## derivative of loss with respect to model parameters ### dloss/dparam\n",
        "#   optimizer.apply_gradients(zip(gradients,model.trainable_variables)) ## updating model weights\n",
        "\n",
        "#   #error = avg_error(heatmaps,predicted_heatmaps)\n",
        "\n",
        "#   return loss.numpy().mean()#,error]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZfKnyi2Qpif"
      },
      "source": [
        "# def get_batch(images,heatmaps,batch_size):\n",
        "#   scale1,scale2,scale3,hmp = shuffle(images[0],images[1],images[2],heatmaps)\n",
        "#   number_of_batches = int(hmp.shape[0]/batch_size)\n",
        "#   start = 0\n",
        "#   end = batch_size\n",
        "#   for i in range(number_of_batches):\n",
        "#     training_data = [scale1[start:end],scale2[start:end],scale3[start:end]]\n",
        "#     training_heatmaps = hmp[start:end]\n",
        "#     yield (training_data,training_heatmaps)\n",
        "#     start+=batch_size\n",
        "#     end += batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUdWYAHz4R_j"
      },
      "source": [
        "# loss_history = []\n",
        "# val_loss_history = []\n",
        "\n",
        "# model = Model([64,128,256],(5,5),kp,regression_filter=512)\n",
        "# optimizer = tf.keras.optimizers.Adam()\n",
        "# loss_object = tf.keras.losses.BinaryCrossentropy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpqrzW62PJi3"
      },
      "source": [
        "# for i in range(epochs):\n",
        "#   ### get random batches ###\n",
        "#   for batch_number, (batch_img, batch_hmp) in enumerate(get_batch(X_train,heatmaps,14)):\n",
        "#     train_loss = train_step(model,optimizer,loss_object,batch_img,batch_hmp) ## mean of batch training loss\n",
        "#     loss_history.append(train_loss)\n",
        "#   print(\"last batch loss for epoch number {0}  is {1}\".format(i,train_loss))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRTH3AEC9IfM"
      },
      "source": [
        "### you could avoid using keras by using the training loop above (takes more memory & better with colab pro) ###\n",
        "model = Model([64,128,256],(5,5),kp,regression_filter=512)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss=tf.keras.losses.binary_crossentropy,metrics=[avg_error],run_eagerly=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZFaqCfvQ9Myw"
      },
      "source": [
        "history = model.fit(X_train,heatmaps,epochs=33,verbose=True,batch_size=14,\n",
        "                    validation_data=(X_test, test_heatmaps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZWNTqjMAYMm"
      },
      "source": [
        "# **Model Evaluation & Inspecting Results**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9cH1i4Z9T8P"
      },
      "source": [
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.plot(history.history['loss'],scalex=True, scaley=True)\n",
        "plt.plot(history.history['val_loss'],scalex=True, scaley=True)\n",
        "plt.savefig(hmaps+\".pdf\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgC21-KSk667"
      },
      "source": [
        "plt.title('model error')\n",
        "plt.ylabel('error')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.plot(history.history['avg_error'],scalex=True, scaley=True)\n",
        "plt.plot(history.history['val_avg_error'],scalex=True, scaley=True)\n",
        "plt.savefig(hmaps+\".pdf\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQVBokvqCnmL"
      },
      "source": [
        "key_points_error = []\n",
        "\n",
        "for i in range(heatmaps.shape[-1]):\n",
        "  key_points_error.append(0)\n",
        "\n",
        "key_points_error = np.array(key_points_error)\n",
        "\n",
        "\n",
        "for i in range(X_test[0].shape[0]):\n",
        "  prediction = model.predict([np.expand_dims(X_test[0][i],axis=0),\n",
        "                              np.expand_dims(X_test[1][i],axis=0),\n",
        "                              np.expand_dims(X_test[2][i],axis=0)])#[1]\n",
        "\n",
        "  t_map = test_heatmaps[i,:,:]\n",
        "  for k in range(prediction.shape[-1]):\n",
        "    pred_map = prediction[0,:,:,k].copy()\n",
        "    truth_map = t_map[:,:,k].copy()\n",
        "\n",
        "    r_pred,c_pred = np.unravel_index(pred_map.argmax(),pred_map.shape)\n",
        "    r_true,c_true = np.unravel_index(truth_map.argmax(),truth_map.shape)\n",
        "    \n",
        "    dy = tf.abs(r_true-r_pred)\n",
        "    dx = tf.abs(c_true-c_pred)\n",
        "    \n",
        "    error = dx+dy\n",
        "    key_points_error[k]+=error\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcSGXLcPDDD2"
      },
      "source": [
        "%matplotlib inline\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "key_point_index = [i for i in range(heatmaps.shape[-1])]\n",
        "#print(key_points_error)\n",
        "ax.bar(key_point_index,key_points_error/X_test[0].shape[0])\n",
        "\n",
        "plt.savefig(hmaps+\"AE.jpg\",bbox_inches='tight',dpi=150)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlfCJNu4SgXy"
      },
      "source": [
        "key_points_error = []\n",
        "for k in range(10):\n",
        "  thresh=[]\n",
        "  for i in range(heatmaps.shape[-1]):\n",
        "    thresh.append(0)\n",
        "  key_points_error.append(thresh)\n",
        "\n",
        "key_points_error = np.array(key_points_error)\n",
        "\n",
        "\n",
        "for i in range(X_test[0].shape[0]):\n",
        "  prediction = model.predict([np.expand_dims(X_test[0][i],axis=0),\n",
        "                              np.expand_dims(X_test[1][i],axis=0),\n",
        "                              np.expand_dims(X_test[2][i],axis=0)])\n",
        "  \n",
        "  t_map = test_heatmaps[i,:,:]\n",
        "  for k in range(prediction.shape[-1]):\n",
        "    pred_map = prediction[0,:,:,k].copy()\n",
        "    truth_map = t_map[:,:,k].copy()\n",
        "\n",
        "    r_pred,c_pred = np.unravel_index(pred_map.argmax(),pred_map.shape)\n",
        "    r_true,c_true = np.unravel_index(truth_map.argmax(),truth_map.shape)\n",
        "    \n",
        "    dy = tf.abs(r_true-r_pred)\n",
        "    dx = tf.abs(c_true-c_pred)\n",
        "    error = dx+dy\n",
        "\n",
        "    for thresh in range(10):\n",
        "      if error<=thresh+1:\n",
        "        key_points_error[thresh][k]+=1\n",
        "key_points_error = np.array(key_points_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYagpySzbt_T"
      },
      "source": [
        "plt.plot([i+1 for i in range(10)],(key_points_error[:,5]/X_test[0].shape[0]))\n",
        "plt.xlabel(\"threshold\")\n",
        "plt.ylabel(\"PCK\")\n",
        "plt.savefig(hmaps+\"PCK.jpg\",bbox_inches='tight',dpi=600)\n",
        "plt.show()\n",
        "#plt.plot([i+1 for i in range(10)],(key_points_error[:,1]/X_test[0].shape[0]))\n",
        "#plt.plot([i+1 for i in range(10)],(key_points_error[:,2]/X_test[0].shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PykDf6cAKWk"
      },
      "source": [
        "start = 30\n",
        "end = start+64\n",
        "prediction = model.predict([X_test[0][start:end],X_test[1][start:end],X_test[2][start:end]])#[1]\n",
        "print(prediction.shape)\n",
        "print(X_test[0][start:end].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWnIqHL7APeV"
      },
      "source": [
        "for i in range(20,50):\n",
        "  visualize_heatmap(X_test[0][start:end],prediction,i,-1,all=True,resize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxTDg6g5T2Bg"
      },
      "source": [
        "# **Building Reconstruction Model**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1titD23CYwa"
      },
      "source": [
        "if path_index == 0:\n",
        "  Y_train = Y_points_train[0][:,:kp].copy().astype(np.float32)##(B,10,2)\n",
        "  Y_test = Y_points_test[0][:,:kp].copy().astype(np.float32)\n",
        "\n",
        "  Y_train[:,0] = Y_points_train[0][:,1].copy()\n",
        "  Y_train[:,1] = Y_points_train[0][:,6].copy()\n",
        "  Y_train[:,2] = Y_points_train[0][:,5].copy()\n",
        "  Y_train[:,3] = Y_points_train[0][:,0].copy()\n",
        "  Y_train[:,4] = Y_points_train[0][:,3].copy()\n",
        "  Y_train[:,5] = Y_points_train[0][:,8].copy()\n",
        "  Y_train[:,6] = Y_points_train[0][:,7].copy()\n",
        "  Y_train[:,7] = Y_points_train[0][:,2].copy()\n",
        "  Y_train[:,8] = Y_points_train[0][:,4].copy()\n",
        "\n",
        "\n",
        "\n",
        "  Y_test[:,0] = Y_points_test[0][:,1].copy()\n",
        "  Y_test[:,1] = Y_points_test[0][:,6].copy()\n",
        "  Y_test[:,2] = Y_points_test[0][:,5].copy()\n",
        "  Y_test[:,3] = Y_points_test[0][:,0].copy()\n",
        "  Y_test[:,4] = Y_points_test[0][:,3].copy()\n",
        "  Y_test[:,5] = Y_points_test[0][:,8].copy()\n",
        "  Y_test[:,6] = Y_points_test[0][:,7].copy()\n",
        "  Y_test[:,7] = Y_points_test[0][:,2].copy()\n",
        "  Y_test[:,8] = Y_points_test[0][:,4].copy()\n",
        "  \n",
        "elif path_index == 2:\n",
        "  Y_train = Y_points_train[0][:,:kp].copy().astype(np.float32)##(B,10,2)\n",
        "  Y_test = Y_points_test[0][:,:kp].copy().astype(np.float32)\n",
        "\n",
        "  Y_train[:,0] = Y_points_train[0][:,1].copy()\n",
        "  Y_train[:,1] = Y_points_train[0][:,8].copy()\n",
        "  Y_train[:,2] = Y_points_train[0][:,7].copy()\n",
        "  Y_train[:,3] = Y_points_train[0][:,0].copy()\n",
        "  Y_train[:,4] = Y_points_train[0][:,3].copy()\n",
        "  Y_train[:,5] = Y_points_train[0][:,10].copy()\n",
        "  Y_train[:,6] = Y_points_train[0][:,9].copy()\n",
        "  Y_train[:,7] = Y_points_train[0][:,2].copy()\n",
        "  Y_train[:,8] = Y_points_train[0][:,6].copy()\n",
        "  Y_train[:,9] = Y_points_train[0][:,13].copy()\n",
        "  Y_train[:,10] = Y_points_train[0][:,4].copy()\n",
        "  Y_train[:,11] = Y_points_train[0][:,5].copy()\n",
        "  Y_train[:,12] = Y_points_train[0][:,11].copy()\n",
        "  Y_train[:,13] = Y_points_train[0][:,12].copy()\n",
        "\n",
        "\n",
        "  Y_test[:,0] = Y_points_test[0][:,1].copy()\n",
        "  Y_test[:,1] = Y_points_test[0][:,8].copy()\n",
        "  Y_test[:,2] = Y_points_test[0][:,7].copy()\n",
        "  Y_test[:,3] = Y_points_test[0][:,0].copy()\n",
        "  Y_test[:,4] = Y_points_test[0][:,3].copy()\n",
        "  Y_test[:,5] = Y_points_test[0][:,10].copy()\n",
        "  Y_test[:,6] = Y_points_test[0][:,9].copy()\n",
        "  Y_test[:,7] = Y_points_test[0][:,2].copy()\n",
        "  Y_test[:,8] = Y_points_test[0][:,6].copy()\n",
        "  Y_test[:,9] = Y_points_test[0][:,13].copy()\n",
        "  Y_test[:,10] = Y_points_test[0][:,4].copy()\n",
        "  Y_test[:,11] = Y_points_test[0][:,5].copy()\n",
        "  Y_test[:,12] = Y_points_test[0][:,11].copy()\n",
        "  Y_test[:,13] = Y_points_test[0][:,12].copy()\n",
        "else:\n",
        "  Y_train = Y_points_train[0].copy().astype(np.float32)\n",
        "  Y_test = Y_points_test[0].copy().astype(np.float32)\n",
        "### translate points to center then normalize\n",
        "\n",
        "translation_value = 0\n",
        "test_translation_value = 0\n",
        "\n",
        "if path_index == 0:\n",
        "  xt1 = Y_train[:,2,0]\n",
        "  xt2 = Y_train[:,8,0]\n",
        "  mid_obj = (xt1+xt2)/2\n",
        "  mid_obj = mid_obj.astype(\"int32\")\n",
        "  translation_value = np.expand_dims(int(HEATMAP_SIZE[1]/2)-mid_obj,axis=-1)\n",
        "  \n",
        "  xt1 = Y_test[:,2,0]\n",
        "  xt2 = Y_test[:,8,0]\n",
        "  mid_obj = (xt1+xt2)/2\n",
        "  mid_obj = mid_obj.astype(\"int32\")\n",
        "  test_translation_value = np.expand_dims(int(HEATMAP_SIZE[1]/2)-mid_obj,axis=-1)\n",
        "\n",
        "elif path_index == 1:\n",
        "  xt1 = Y_train[:,8,0]\n",
        "  xt2 = Y_train[:,9,0]\n",
        "  mid_obj = (xt1+xt2)/2\n",
        "  mid_obj = mid_obj.astype(\"int32\")\n",
        "  translation_value = np.expand_dims(int(HEATMAP_SIZE[1]/2)-mid_obj,axis=-1)\n",
        "\n",
        "  xt1 = Y_test[:,8,0]\n",
        "  xt2 = Y_test[:,9,0]\n",
        "  mid_obj = (xt1+xt2)/2\n",
        "  mid_obj = mid_obj.astype(\"int32\")\n",
        "  test_translation_value = np.expand_dims(int(HEATMAP_SIZE[1]/2)-mid_obj,axis=-1)\n",
        "\n",
        "\n",
        "# Y_train[:,:,0] +=translation_value \n",
        "\n",
        "Y_train[:,:,0]/=15\n",
        "Y_train[:,:,0]-=1\n",
        "\n",
        "Y_train[:,:,1]/=20\n",
        "Y_train[:,:,1]-=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Y_test[:,:,0] += test_translation_value \n",
        "Y_test[:,:,0]/=15\n",
        "Y_test[:,:,0]-=1\n",
        "\n",
        "Y_test[:,:,1]/=20\n",
        "Y_test[:,:,1]-=1\n",
        "\n",
        "print(\"minimum in normalized X train is {}\".format(Y_train[:,:,0].min()))\n",
        "print(\"maximum in normalized X train is {}\".format(Y_train[:,:,0].max()))\n",
        "\n",
        "print(\"minimum in normalized Y train is {}\".format(Y_train[:,:,1].min()))\n",
        "print(\"maximum in normalized Y train is {}\".format(Y_train[:,:,1].max()))\n",
        "\n",
        "print(\"minimum in normalized X test is {}\".format(Y_test[:,:,0].min()))\n",
        "print(\"maximum in normalized X test is {}\".format(Y_test[:,:,0].max()))\n",
        "\n",
        "print(\"minimum in normalized Y test is {}\".format(Y_test[:,:,1].min()))\n",
        "print(\"maximum in normalized Y test is {}\".format(Y_test[:,:,1].max()))\n",
        "\n",
        "Y_train = Y_train.reshape(-1,kp*2)\n",
        "Y_test = Y_test.reshape(-1,kp*2)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMKZ2mDt2Y2i"
      },
      "source": [
        "def inspect_points(points,category):\n",
        "  line_thickness = 1\n",
        "  edges = dict()\n",
        "\n",
        "  edges[0] = [[1,5],[2,6],[3,7],[4,8],[3,4],[5,6],[6,7],[7,8],[5,8],[8,9],[7,10],[9,10]]\n",
        "  edges[1] = [[1,5],[2,6],[3,7],[4,8],[5,6],[6,7],[7,8],[5,8],[8,9],[7,10],[9,10]]\n",
        "  edges[2] = [[1,5],[2,6],[3,7],[4,8],[5,6],[6,7],[7,8],[5,8],[8,9],[7,10],[9,10],[11,12],[12,5],[13,14],[14,6]]\n",
        "  edges[3] = [[1,6],[2,6],[3,6],[4,6],[5,6],[6,7],[8,9],[9,10],[8,11],[10,11],[11,12],[12,13],[10,13]] \n",
        "  #edges[4] =\n",
        "\n",
        "  if points.shape[-1]!=3:\n",
        "    x = points.reshape(kp,2).copy()\n",
        "  else:\n",
        "    x = points.reshape(kp,3).copy()\n",
        "  \n",
        "  image  = np.full((40,30,3),255,dtype=np.float32)## creation of white image\n",
        "  \n",
        "  print(x.shape)\n",
        "  x[:,0]+=1\n",
        "  x[:,0]*=15\n",
        "  x[:,1]+=1\n",
        "  x[:,1]*=20\n",
        "\n",
        "  for edge in edges[category]:\n",
        "      p1 = x[edge[0]-1]\n",
        "      p2 = x[edge[1]-1]\n",
        "      \n",
        "      cv.line(image, (int(p1[0]), int(p1[1])), (int(p2[0]), int(p2[1])), (0, 0, 0), thickness=line_thickness)\n",
        "\n",
        "\n",
        "  plt.imshow(image,cmap=\"gray\")\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p_QJbYU2ZiZ"
      },
      "source": [
        "index = 0\n",
        "inspect_points(Y_test[index],path_index)\n",
        "plt.figure()\n",
        "plt.imshow(X_test[0][index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Suexl6H0Vrkz"
      },
      "source": [
        "def inspect(weights,rotations,focus,to_rad=True,category=0):\n",
        "  deformations = list()\n",
        "  scale = 1.2\n",
        "  def rotate(theta,axis):\n",
        "    rad = theta\n",
        "    if to_rad:\n",
        "      rad = (theta/180)*math.pi\n",
        "\n",
        "    rotation = {}\n",
        "    rotation[0] = [[1,0,0,0],\n",
        "                  [0,math.cos(rad),-math.sin(rad),0],\n",
        "                  [0,math.sin(rad),math.cos(rad),0],\n",
        "                  [0,0,0,1]]\n",
        "\n",
        "    rotation[1] = [[math.cos(rad),0,math.sin(rad),0],\n",
        "                  [0,1,0,0],\n",
        "                  [-math.sin(rad),0,math.cos(rad),0],\n",
        "                  [0,0,0,1]]\n",
        "\n",
        "    rotation[2] = [[math.cos(rad),-math.sin(rad),0,0],\n",
        "                  [math.sin(rad),math.cos(rad),0,0],\n",
        "                  [0,0,1,0],\n",
        "                  [0,0,0,1]]\n",
        "\n",
        "    return np.array(rotation[axis],dtype=np.float32)\n",
        "  \n",
        "  if category == 0:\n",
        "    mean_shape =  np.array([[-1,-1,2],\n",
        "                           [ 1,-1,2],\n",
        "                           [1,-1,-2],\n",
        "                           [-1,-1,-2],\n",
        "                           [-1,0,2],\n",
        "                           [1,0,2], \n",
        "                           [1,0,-2], \n",
        "                           [-1,0,-2],\n",
        "                           [-1,1,-2],\n",
        "                           [1,1,-2]],dtype=np.float32)\n",
        "\n",
        "    edges = [[1,5],[2,6],[3,7],[4,8],[3,4],[5,6],[6,7],[7,8],[5,8],[8,9],[7,10],[9,10]]\n",
        "    d0 = np.array([[0,0,1],[0,0,1],[0,0,-1],[0,0,-1],[0,0,1],[0,0,1],[0,0,-1],[0,0,-1],[0,0,-1],[0,0,-1]])\n",
        "    d1 = np.array([[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0]])\n",
        "    d2 = np.array([[0,-1,0],[0,-1,0],[0,-1,0],[0,-1,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "    d3 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,1,0],[0,1,0]])\n",
        "\n",
        "    deformations.append(d0)\n",
        "    deformations.append(d1)\n",
        "    deformations.append(d2)\n",
        "    deformations.append(d3)\n",
        "\n",
        "  elif category == 1:\n",
        "    mean_shape = np.array([[-1,-2,1],\n",
        "                          [1,-2,1],\n",
        "                          [1,-2,-1],\n",
        "                          [-1,-2,-1],\n",
        "                          [-1,0,1],\n",
        "                          [1,0,1],\n",
        "                          [1,0,-1],\n",
        "                          [-1,0,-1],\n",
        "                          [-1,2,-1],\n",
        "                          [1,2,-1]],dtype=np.float32)\n",
        "    \n",
        "    edges = [[1,5],[2,6],[3,7],[4,8],[5,6],[6,7],[7,8],[5,8],[8,9],[7,10],[9,10]]\n",
        "    d0 = np.array([[0,-1,0],[0,-1,0],[0,-1.,0],[0,-1,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0], [0,0,0],[0,0,0]])\n",
        "    d1 =  np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0.,0],[0,0,0],[0,1,0],[0,1,0]])\n",
        "    d2 = np.array([[-1.,0,0],[1.,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0]])\n",
        "    d3 =  np.array([[-1.,0,1],[1,0,1],[1,0,-1],[-1.,0,-1],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "\n",
        "    deformations.append(d0)\n",
        "    deformations.append(d1)\n",
        "    deformations.append(d2)\n",
        "    deformations.append(d3)\n",
        "\n",
        "  elif category == 2:\n",
        "    mean_shape = np.array([[-2,-2,1],\n",
        "                          [2,-2,1],\n",
        "                          [2,-2,-1],\n",
        "                          [-2,-2,-1],\n",
        "                          [-2,0,1], \n",
        "                          [2,0,1],\n",
        "                          [2,0,-1],\n",
        "                          [-2,0,-1],\n",
        "                          [-2,2,-1],\n",
        "                          [2,2,-1],\n",
        "                          [-2,1,-1],\n",
        "                          [-2,1,1],\n",
        "                          [2,1,-1],\n",
        "                          [2,1,1]],dtype=np.float32)\n",
        "    \n",
        "    edges = [[1,5],[2,6],[3,7],[4,8],[5,6],[6,7],[7,8],[5,8],[8,9],[7,10],[9,10],[11,12],[12,5],[13,14],[14,6]]\n",
        "    d0 = np.array([[0,0,1],[0,0,1],[0,0,-1],[0,0,-1],[0,0,1],[0,0,1],[0,0,-1],[0,0,-1],[0,0,-1],[0,0,-1],[0,0,-1],[0,0,1],[0,0,-1],[0,0,1]])\n",
        "    d1 = np.array([[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0],[1,0,0]])\n",
        "    d2 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,1,0],[0,1,0],[0,0.5,0],[0,0.5,0],[0,0.5,0],[0,0.5,0]])\n",
        "    d3 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0]])\n",
        "    d4 = np.array([[0,-1,0],[0,-1,0],[0,-1,0],[0,-1,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "    d5 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,-1],[0,0,0],[0,0,-1]])\n",
        "\n",
        "    deformations.append(d0)\n",
        "    deformations.append(d1)\n",
        "    deformations.append(d2)\n",
        "    deformations.append(d3)\n",
        "    deformations.append(d4)\n",
        "    deformations.append(d5)\n",
        "\n",
        "  elif category == 3:\n",
        "    mean_shape = np.array([[0.309*scale,-1.5,0.9511*scale],\n",
        "                           [-0.809*scale,-1.5,0.5878*scale],\n",
        "                           [-0.809*scale,-1.5,-0.5878*scale], \n",
        "                           [0.309*scale,-1.5,-0.9511*scale],\n",
        "                           [1*scale,-1.5,0.0000*scale],\n",
        "                           [0,-1.5,0],\n",
        "                           [0,0,0],\n",
        "                           [-1,0,1],\n",
        "                           [1,0,1],\n",
        "                           [1,0,-1],\n",
        "                           [-1,0,-1],\n",
        "                           [-1,1.5,-1],\n",
        "                           [1,1.5,-1]],dtype=np.float32)\n",
        "   \n",
        "    edges = [[1,6],[2,6],[3,6],[4,6],[5,6],[6,7],[8,9],[9,10],[8,11],[10,11],[11,12],[12,13],[10,13]]\n",
        "    d0 = np.array([[0,-1,0],[0,-1,0],[0,-1,0],[0,-1,0],[0,-1,0],[0,-1,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "    d1 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,1,0],[0,1,0]])\n",
        "    d2 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0]])\n",
        "    d3 = np.array([[0.309,0,0.9511],[-0.809,0,0.5878],[-0.809,0,-0.5878],[0.309,0,-0.9511],[1,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "    d4 = np.array([[-0.9511,0,0.309],[-0.5878,0,-0.809],[0.5878,0,-0.809],[0.9511,0,0.309],[0,0,1],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "    d5 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,1,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "\n",
        "    deformations.append(d0)\n",
        "    deformations.append(d1)\n",
        "    deformations.append(d2)\n",
        "    deformations.append(d3)\n",
        "    deformations.append(d4)\n",
        "    deformations.append(d5)\n",
        "\n",
        "  line_thickness = 1\n",
        "  image  = np.full((40,30,3),255,dtype=np.float32)## creation of white image\n",
        "\n",
        "  rx,ry,rz = rotations\n",
        "  #tx,ty,tz = translations\n",
        "  weighted_shape = mean_shape.copy()\n",
        "  for i in range(len(deformations)):\n",
        "    weighted_shape+=(deformations[i]*weights[i])\n",
        "\n",
        "\n",
        "  mean_shape = weighted_shape.copy()\n",
        "  \n",
        "  points = mean_shape.T\n",
        "  ones = np.expand_dims(np.ones(points.shape[-1]),axis=0)\n",
        "  points = np.concatenate([points,ones],axis=0)\n",
        "\n",
        "  Rx = rotate(rx,axis=0) ## X-axis\n",
        "  Ry = rotate(ry,axis=1) ## Y-axis\n",
        "  Rz = rotate(rz,axis=2) ## Z-axis\n",
        "  Rxy = np.dot(Rx,Ry)## \n",
        "\n",
        "  R = np.dot(Rxy,Rz)### rotation matrix\n",
        "\n",
        "  points = np.dot(R,points) ## Rx+T\n",
        "\n",
        "  points[2]-=10\n",
        "\n",
        "  points = points/((1/focus)*points[2]+1)\n",
        "\n",
        "  points[0,:]+=1\n",
        "  points[0,:]*=15\n",
        "  points[1,:]+=1\n",
        "  points[1,:]*=20\n",
        "  points = points.T\n",
        "\n",
        "  for edge in edges:\n",
        "      p1 = points[edge[0]-1]\n",
        "      p2 = points[edge[1]-1]\n",
        "      \n",
        "      cv.line(image, (round(p1[0]), round(p1[1])), (round(p2[0]), round(p2[1])), (0, 0, 0), thickness=line_thickness)\n",
        "\n",
        "  plt.imshow(image,cmap=\"gray\")\n",
        "  return points.astype(\"int32\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gzr7W87_B1em"
      },
      "source": [
        "inspect([0,0,0,0,0,0,0],[40,80,0],2,category=path_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cH-LmeoRKn2"
      },
      "source": [
        "def pj_error_wrapper(kp,deform_count,category=0):\n",
        "  def projection_error(y_true,y_pred):\n",
        "    \n",
        "    deformations = list()\n",
        "\n",
        "    if category == 0:\n",
        "      mean_shape =  np.array([[-1,-1,2],\n",
        "                           [ 1,-1,2],\n",
        "                           [1,-1,-2],\n",
        "                           [-1,-1,-2],\n",
        "                           [-1,0,2],\n",
        "                           [1,0,2], \n",
        "                           [1,0,-2], \n",
        "                           [-1,0,-2],\n",
        "                           [-1,1,-2],\n",
        "                           [1,1,-2]],dtype=np.float32)\n",
        "\n",
        "      d0 = np.array([[0,0,1],[0,0,1],[0,0,-1],[0,0,-1],[0,0,1],[0,0,1],[0,0,-1],[0,0,-1],[0,0,-1],[0,0,-1]])\n",
        "      d1 = np.array([[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0]])\n",
        "      d2 = np.array([[0,-1,0],[0,-1,0],[0,-1,0],[0,-1,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "      d3 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,1,0],[0,1,0]])\n",
        "\n",
        "      deformations.append(d0)\n",
        "      deformations.append(d1)\n",
        "      deformations.append(d2)\n",
        "      deformations.append(d3)\n",
        "\n",
        "    elif category == 1:\n",
        "      mean_shape = np.array([[-1,-2,1],\n",
        "                            [1,-2,1],\n",
        "                            [1,-2,-1],\n",
        "                            [-1,-2,-1],\n",
        "                            [-1,0,1],\n",
        "                            [1,0,1],\n",
        "                            [1,0,-1],\n",
        "                            [-1,0,-1],\n",
        "                            [-1,2,-1],\n",
        "                            [1,2,-1]],dtype=np.float32)\n",
        "\n",
        "      d0 = np.array([[0,-1,0],[0,-1,0],[0,-1.,0],[0,-1,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0], [0,0,0],[0,0,0]])\n",
        "      d1 =  np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0.,0],[0,0,0],[0,1,0],[0,1,0]])\n",
        "      d2 = np.array([[-1.,0,0],[1.,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0]])\n",
        "      d3 =  np.array([[-1.,0,1],[1,0,1],[1,0,-1],[-1.,0,-1],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "\n",
        "      deformations.append(d0)\n",
        "      deformations.append(d1)\n",
        "      deformations.append(d2)\n",
        "      deformations.append(d3)\n",
        "      \n",
        "    elif category == 2:\n",
        "      mean_shape = np.array([[-2,-2,1],\n",
        "                          [2,-2,1],\n",
        "                          [2,-2,-1],\n",
        "                          [-2,-2,-1],\n",
        "                          [-2,0,1], \n",
        "                          [2,0,1],\n",
        "                          [2,0,-1],\n",
        "                          [-2,0,-1],\n",
        "                          [-2,2,-1],\n",
        "                          [2,2,-1],\n",
        "                          [-2,1,-1],\n",
        "                          [-2,1,1],\n",
        "                          [2,1,-1],\n",
        "                          [2,1,1]],dtype=np.float32)\n",
        "      \n",
        "      d0 = np.array([[0,0,1],[0,0,1],[0,0,-1],[0,0,-1],[0,0,1],[0,0,1],[0,0,-1],[0,0,-1],[0,0,-1],[0,0,-1],[0,0,-1],[0,0,1],[0,0,-1],[0,0,1]])\n",
        "      d1 = np.array([[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0],[1,0,0]])\n",
        "      d2 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,1,0],[0,1,0],[0,0.5,0],[0,0.5,0],[0,0.5,0],[0,0.5,0]])\n",
        "      d3 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0]])\n",
        "      d4 = np.array([[0,-1,0],[0,-1,0],[0,-1,0],[0,-1,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "      d5 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,-1],[0,0,0],[0,0,-1]])\n",
        "\n",
        "      deformations.append(d0)\n",
        "      deformations.append(d1)\n",
        "      deformations.append(d2)\n",
        "      deformations.append(d3)\n",
        "      deformations.append(d4)\n",
        "      deformations.append(d5)\n",
        "\n",
        "    elif category == 3:\n",
        "      scale=1.2\n",
        "      mean_shape = np.array([[0.309*scale,-1.5,0.9511*scale],\n",
        "                            [-0.809*scale,-1.5,0.5878*scale],\n",
        "                            [-0.809*scale,-1.5,-0.5878*scale], \n",
        "                            [0.309*scale,-1.5,-0.9511*scale],\n",
        "                            [1*scale,-1.5,0.0000*scale],\n",
        "                            [0,-1.5,0],\n",
        "                            [0,0,0],\n",
        "                            [-1,0,1],\n",
        "                            [1,0,1],\n",
        "                            [1,0,-1],\n",
        "                            [-1,0,-1],\n",
        "                            [-1,1.5,-1],\n",
        "                            [1,1.5,-1]],dtype=np.float32)\n",
        "    \n",
        "      edges = [[1,6],[2,6],[3,6],[4,6],[5,6],[6,7],[8,9],[9,10],[8,11],[10,11],[11,12],[12,13],[10,13]]\n",
        "      d0 = np.array([[0,-1,0],[0,-1,0],[0,-1,0],[0,-1,0],[0,-1,0],[0,-1,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "      d1 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,1,0],[0,1,0]])\n",
        "      d2 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0]])\n",
        "      d3 = np.array([[0.309,0,0.9511],[-0.809,0,0.5878],[-0.809,0,-0.5878],[0.309,0,-0.9511],[1,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "      d4 = np.array([[-0.9511,0,0.309],[-0.5878,0,-0.809],[0.5878,0,-0.809],[0.9511,0,0.309],[0,0,1],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "      d5 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,1,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "\n",
        "      deformations.append(d0)\n",
        "      deformations.append(d1)\n",
        "      deformations.append(d2)\n",
        "      deformations.append(d3)\n",
        "      deformations.append(d4)\n",
        "      deformations.append(d5)\n",
        "      \n",
        "    data = y_pred\n",
        "    loss=0\n",
        "    for i in range(data.shape[0]):\n",
        "      y_pred=data[i]\n",
        "      weights,R,focus = y_pred[:deform_count],y_pred[deform_count:deform_count+9],y_pred[deform_count+9]  \n",
        "      R = tf.reshape(R,[3,3])\n",
        "\n",
        "      weighted_shape = mean_shape.copy()\n",
        "      for k in range(len(deformations)):\n",
        "        weighted_shape +=(weights[k]*deformations[k])\n",
        "    \n",
        "      points = tf.transpose(weighted_shape)\n",
        "\n",
        "      points = tf.matmul(R,points)\n",
        "\n",
        "      points = tf.stack([points[0],points[1],points[2]-10])\n",
        "      points = points/((1/focus)*points[2]+1)##projection\n",
        "\n",
        "      points = points[0:2]\n",
        "      points = tf.transpose(points)\n",
        "\n",
        "      points = tf.reshape(points,[kp*2])\n",
        "\n",
        "      loss += tf.losses.mean_squared_error(y_true[i],points)#tf.keras.losses.mean_squared_logarithmic_error(y_true[i],points)#\n",
        "    return loss/data.shape[0]\n",
        "\n",
        "  return projection_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_cv2E_2CLyS"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(46)\n",
        "#9 13 20 23 28 38 43 44* 46\n",
        "def construction_model(w,input=Input(shape=(20)),category=0):\n",
        "  ### seed 3 -1 2 5 9 14 15 16 30 33 34\n",
        "  x = Flatten()(input)\n",
        "  if category == 0 or category == 3:\n",
        "    x = Dense(2048,activation=\"relu\")(x)\n",
        "    x = Dense(1024,activation=\"relu\")(x)\n",
        "    x = Dense(512)(x)\n",
        "    x = Dense(256)(x)\n",
        "    x = Dense(128)(x)\n",
        "    x = Dense(10+w)(x)\n",
        "\n",
        "  elif category == 1:    \n",
        "    x = Dense(2048,activation=\"relu\")(x)\n",
        "    x = Dense(512,activation=\"relu\")(x)\n",
        "    x = Dense(128)(x)\n",
        "    x = Dense(14)(x)\n",
        "\n",
        "  elif category == 2:\n",
        "    x = Dense(2048,activation=\"relu\")(x)\n",
        "    x = Dense(512,activation=\"relu\")(x)\n",
        "    x = Dense(128)(x)\n",
        "    x = Dense(16)(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=input,outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65ZUkseGQYNA"
      },
      "source": [
        "model = construction_model(w_ct,Input(shape=(kp*2)),category=path_index)\n",
        "plot_model(model)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ev9nA80P2la"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer = optimizer,loss=pj_error_wrapper(kp,w_ct,category=path_index),run_eagerly=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-jrkvreWGEg"
      },
      "source": [
        "history = model.fit(Y_train,\n",
        "          Y_train,\n",
        "          validation_data=(Y_test,Y_test),\n",
        "          epochs=30, verbose=True, batch_size=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ-k6Q2B3vTC"
      },
      "source": [
        "# ***Construction Model Evaluation and inspecting results***\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEVQCNrMTOH7"
      },
      "source": [
        "!rm -r swivelchair_construct_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1RYXwmnxn9I"
      },
      "source": [
        "model.save(paths[path_index]+\"_construct_weights\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJMrdM_cW7a3"
      },
      "source": [
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.axis([0,30,0,0.1])\n",
        "plt.plot(history.history['loss'],scalex=True, scaley=True)\n",
        "plt.plot(history.history['val_loss'],scalex=True, scaley=True)\n",
        "plt.savefig(hmaps+\".pdf\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aarMIZm-rHLF"
      },
      "source": [
        "def inspect(weights,rotation,focus,refine=False,category=0):\n",
        "\n",
        "  line_thickness = 1\n",
        "  image  = np.full((80,80,3),255,dtype=np.float32)## creation of white image\n",
        "  \n",
        "  deformations = list()\n",
        "  if category == 0:\n",
        "    mean_shape =  np.array([[-1,-1,2],\n",
        "                            [1,-1,2],\n",
        "                            [1,-1,-2],\n",
        "                            [-1,-1,-2],\n",
        "                            [-1,0,2],\n",
        "                            [1,0,2], \n",
        "                            [1,0,-2],\n",
        "                            [-1,0,-2],\n",
        "                            [-1,1,-2],\n",
        "                            [1,1,-2]],dtype=np.float32)\n",
        "    \n",
        "    edges = [[1,5],[2,6],[3,7],[4,8],[3,4],[5,6],[6,7],[7,8],[5,8],[8,9],[7,10],[9,10]]\n",
        "\n",
        "    d0 = np.array([[0,0,1],[0,0,1],[0,0,-1],[0,0,-1],[0,0,1],[0,0,1],[0,0,-1],[0,0,-1],[0,0,-1],[0,0,-1]])\n",
        "    d1 = np.array([[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0]])\n",
        "    d2 = np.array([[0,-1,0],[0,-1,0],[0,-1,0],[0,-1,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "    d3 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,1,0],[0,1,0]])\n",
        "\n",
        "    deformations.append(d0)\n",
        "    deformations.append(d1)\n",
        "    deformations.append(d2)\n",
        "    deformations.append(d3)\n",
        "\n",
        "  elif category == 1:\n",
        "    mean_shape = np.array([[-1,-2,1],\n",
        "                          [1,-2,1],\n",
        "                          [1,-2,-1],\n",
        "                          [-1,-2,-1],\n",
        "                          [-1,0,1],\n",
        "                          [1,0,1],\n",
        "                          [1,0,-1],\n",
        "                          [-1,0,-1],\n",
        "                          [-1,2,-1],\n",
        "                          [1,2,-1]],dtype=np.float32)\n",
        "    \n",
        "    edges = [[1,5],[2,6],[3,7],[4,8],[5,6],[6,7],[7,8],[5,8],[8,9],[7,10],[9,10]]\n",
        "\n",
        "    d0 = np.array([[0,-1,0],[0,-1,0],[0,-1.,0],[0,-1,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0], [0,0,0],[0,0,0]])\n",
        "    d1 =  np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0.,0],[0,0,0],[0,1,0],[0,1,0]])\n",
        "    d2 = np.array([[-1.,0,0],[1.,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0]])\n",
        "    d3 =  np.array([[-1.,0,1],[1,0,1],[1,0,-1],[-1.,0,-1],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "\n",
        "    deformations.append(d0)\n",
        "    deformations.append(d1)\n",
        "    deformations.append(d2)\n",
        "    deformations.append(d3)\n",
        "\n",
        "  elif category == 2:\n",
        "  \n",
        "    mean_shape = np.array([[-2,-2,1],\n",
        "                          [2,-2,1],\n",
        "                          [2,-2,-1],\n",
        "                          [-2,-2,-1],\n",
        "                          [-2,0,1], \n",
        "                          [2,0,1],\n",
        "                          [2,0,-1],\n",
        "                          [-2,0,-1],\n",
        "                          [-2,2,-1],\n",
        "                          [2,2,-1],\n",
        "                          [-2,1,-1],\n",
        "                          [-2,1,1],\n",
        "                          [2,1,-1],\n",
        "                          [2,1,1]],dtype=np.float32)\n",
        "    \n",
        "    edges = [[1,5],[2,6],[3,7],[4,8],[5,6],[6,7],[7,8],[5,8],[8,9],[7,10],[9,10],[11,12],[12,5],[13,14],[14,6]]\n",
        "    d0 = np.array([[0,0,1],[0,0,1],[0,0,-1],[0,0,-1],[0,0,1],[0,0,1],[0,0,-1],[0,0,-1],[0,0,-1],[0,0,-1],[0,0,-1],[0,0,1],[0,0,-1],[0,0,1]])\n",
        "    d1 = np.array([[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0],[1,0,0]])\n",
        "    d2 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,1,0],[0,1,0],[0,0.5,0],[0,0.5,0],[0,0.5,0],[0,0.5,0]])\n",
        "    d3 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0]])\n",
        "    d4 = np.array([[0,-1,0],[0,-1,0],[0,-1,0],[0,-1,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "    d5 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,-1],[0,0,0],[0,0,-1]])\n",
        "\n",
        "    deformations.append(d0)\n",
        "    deformations.append(d1)\n",
        "    deformations.append(d2)\n",
        "    deformations.append(d3)\n",
        "    deformations.append(d4)\n",
        "    deformations.append(d5)\n",
        "\n",
        "  elif category == 3:\n",
        "    scale=1.2\n",
        "    mean_shape = np.array([[0.309*scale,-1.5,0.9511*scale],\n",
        "                          [-0.809*scale,-1.5,0.5878*scale],\n",
        "                          [-0.809*scale,-1.5,-0.5878*scale], \n",
        "                          [0.309*scale,-1.5,-0.9511*scale],\n",
        "                          [1*scale,-1.5,0.0000*scale],\n",
        "                          [0,-1.5,0],\n",
        "                          [0,0,0],\n",
        "                          [-1,0,1],\n",
        "                          [1,0,1],\n",
        "                          [1,0,-1],\n",
        "                          [-1,0,-1],\n",
        "                          [-1,1.5,-1],\n",
        "                          [1,1.5,-1]],dtype=np.float32)\n",
        "  \n",
        "    edges = [[1,6],[2,6],[3,6],[4,6],[5,6],[6,7],[8,9],[9,10],[8,11],[10,11],[11,12],[12,13],[10,13]]\n",
        "    d0 = np.array([[0,-1,0],[0,-1,0],[0,-1,0],[0,-1,0],[0,-1,0],[0,-1,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "    d1 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,1,0],[0,1,0]])\n",
        "    d2 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[-1,0,0],[1,0,0],[1,0,0],[-1,0,0],[-1,0,0],[1,0,0]])\n",
        "    d3 = np.array([[0.309,0,0.9511],[-0.809,0,0.5878],[-0.809,0,-0.5878],[0.309,0,-0.9511],[1,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "    d4 = np.array([[-0.9511,0,0.309],[-0.5878,0,-0.809],[0.5878,0,-0.809],[0.9511,0,0.309],[0,0,1],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "    d5 = np.array([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,1,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]])\n",
        "\n",
        "    deformations.append(d0)\n",
        "    deformations.append(d1)\n",
        "    deformations.append(d2)\n",
        "    deformations.append(d3)\n",
        "    deformations.append(d4)\n",
        "    deformations.append(d5)\n",
        "\n",
        "  for i in range(len(deformations)):\n",
        "    mean_shape+= (weights[i]*deformations[i])\n",
        "\n",
        "  points = mean_shape.T\n",
        "  R = rotation.reshape((3,3))\n",
        "  points = np.dot(R,points) ## Rx+T\n",
        "\n",
        "  points[2]-=10\n",
        "  points = points/((1/focus)*points[2]+1)\n",
        "\n",
        "  points[0,:]+=1\n",
        "  points[0,:]*=35\n",
        "  points[1,:]+=1\n",
        "  points[1,:]*=35\n",
        "  points[0]+=5\n",
        "  points[1]+=5\n",
        "  points = points.T\n",
        "\n",
        "  for edge in edges:\n",
        "      p1 = points[edge[0]-1]\n",
        "      p2 = points[edge[1]-1]\n",
        "      \n",
        "      if refine:\n",
        "        if category == 1:\n",
        "          # if edge == [9,10]:\n",
        "          #   p1[1] = p2[1]-0.5\n",
        "            # p2[1] = p\n",
        "          if edge in [[7,10],[3,7],[1,5],[2,6],[8,9]]:\n",
        "            p1[0] = p2[0]\n",
        "            \n",
        "\n",
        "      cv.line(image, (round(p1[0]), round(p1[1])), (round(p2[0]), round(p2[1])), (0, 0, 0), thickness=line_thickness)\n",
        "\n",
        "  plt.imshow(image,cmap=\"gray\")\n",
        "  return points.astype(\"int32\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_B1-tjJJXIh"
      },
      "source": [
        "model.evaluate(Y_test, Y_test, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu7bO17dKB6S"
      },
      "source": [
        "start=30\n",
        "end = start+14\n",
        "prediction = model.predict(Y_test[start:end])\n",
        "print(prediction.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2VI894EOuA4"
      },
      "source": [
        "i = 3\n",
        "inspect(prediction[i][0:w_ct],prediction[i][w_ct:w_ct+9],prediction[i][w_ct+9],category=path_index,refine=False)\n",
        "rotation_matrix = prediction[i][w_ct:w_ct+9].reshape(3,3)\n",
        "\n",
        "thetax = math.atan2(rotation_matrix[-1,1],rotation_matrix[-1,2]) \n",
        "thetay = math.atan2(-rotation_matrix[-1,0],math.sqrt(rotation_matrix[-1,1]**2+rotation_matrix[-1,2]**2))\n",
        "thetaz = math.atan2(rotation_matrix[1,0],rotation_matrix[0,0])\n",
        "thetax*=(180/math.pi)\n",
        "thetay*=(180/math.pi)\n",
        "thetaz*=(180/math.pi)\n",
        "print(thetax)\n",
        "print(thetay)\n",
        "print(thetaz)\n",
        "plt.figure()\n",
        "plt.imshow(X_test2[start+i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66qb60ee93lf"
      },
      "source": [
        "!zip -r chair_construct_weights.zip chair_construct_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv2zvGO27Qzw"
      },
      "source": [
        "!rm -f swivelchair_construct_weights.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MinwktukTxFr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}